# 基本用法
# 初始化`environment`
在 Gym 中初始化`environment`非常简单，可以通过以下方式完成：

    import gym
    env = gym.make('CartPole-v0')
# 与`environment`互动
Gym 施行了经典的 “**agent-environment loop**“：
<div style="text-align: center;">
<img src="images/AE_loop.png" alt="" style="width: 400px">
</div>

`agent`在`environment`中执行一些操作（通常是将一些控制输入传递给`environment`，例如电机的扭矩输入）并观察`environment`的状态如何变化。
这样的一种`action-observation`交互被称为`timestep`。

RL的目标是以某种特定方式操控`environment`。例如，我们希望`agent`将机器人导航到空间中的特定点。 
如果它实现了这一点（或者取得了一些进展），它将在这个`timestep`的观察中获得正的`reward`。 
如果`agent`尚未成功（或未取得任何进展），则`reward`也可能为负或零。然后对`agent`进行训练，
以最大化它在很多个`timestep`中累计的奖励。

经过一些`timestep`后，`environment`可能会进入`terminal`状态。例如，机器人可能坠毁了！ 
在这种情况下，我们希望将`environment`重置为新的`initial`状态。如果`agent`进入这样的`terminal`状态，
`environment`会向`agent`发出`done`信号。并非所有`done`信号都必须由“灾难性事故”触发。
有时我们还希望在固定数量的`timestep`或者`agent`已成功完成`environment`中的某些任务后发出`done`信号。