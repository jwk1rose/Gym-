# 基本用法
## 初始化环境
在 Gym 中初始化环境非常简单，可以通过以下方式完成：

    import gym
    env = gym.make('CartPole-v0')
## 与环境互动
Gym 施行了经典的 `agent-environment loop`：
<div style="text-align: center;">
<img src="images/AE_loop.png" alt="" style="width: 400px">
</div>

`agent`在环境中执行一些操作（通常是将一些控制输入传递给环境，例如电机的扭矩输入）并观察环境的状态如何变化。
这样的一种`action-observation`交互被称为`timestep`。

RL的目标是以某种特定方式操控环境。例如，我们希望`agent`将机器人导航到空间中的特定点。 
如果它实现了这一点（或者取得了一些进展），它将在这个`timestep`的观察中获得正的`reward`。 
如果`agent`尚未成功（或未取得任何进展），则`reward`也可能为负或零。然后对`agent`进行训练，
以最大化它在很多个`timestep`中累计的奖励。

经过一些`timestep`后，环境可能会进入`terminal`状态。例如，机器人可能坠毁了！ 
在这种情况下，我们希望将环境重置为新的`initial`状态。如果`agent`进入这样的`terminal`状态，
环境会向`agent`发出`done`信号。并非所有`done`信号都必须由“灾难性事故”触发。
有时我们还希望在固定数量的`timestep`或者`agent`已成功完成环境中的某些任务后发出`done`信号。

让我们看看 Gym 中的`agent-environment loop`是什么样的。此示例将运行 `LunarLander-v2` 环境实例 1000 个`time-step`。
由于我们传递了`render_mode="human"`，您应该会看到一个渲染环境的窗口弹出。

    # test 1.1
    import gym
    env = gym.make("LunarLander-v2", render_mode="human")
    env.action_space.seed(42)

    observation, info = env.reset(seed=42)

    for _ in range(1000):
    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())

    if terminated or truncated:
        observation, info = env.reset()

    env.close()
输出应该是这样的：
<div style="text-align: center;">
<img src='images/pic1.gif' alt="" style="width: 400px">
</div>

每个环境都通过提供 `env.action_space` 属性来指定有效`action`的格式。同样，有效的`observations`的格式由 env.observation_space指定。
在上面的示例中，我们通过 `env.action_space.sample()` 对随机`action`进行了采样。
请注意，我们需要将动作空间与环境分开`seed`,以确保样本的可重现性。

## 检查API一致性
如果您已经实现了自定义环境并希望执行健全性检查以确保它符合API，您可以运行：
    
    >>> from gym.utils.env_checker import check_env
    >>> check_env(env)
如果您的环境不遵循 Gym API规范，此函数将抛出异常。 
如果你犯了错误或没有遵循最佳实现（例如，如果 `observation_space` 看起来像图像但没有正确的数据类型），
它会产生警告。可以通过传递 `warn=False` 来关闭警告。 
默认情况下，`check_env` 不会检查渲染方法。 要更改此行为，您可以传递 `skip_render_check=False`。

## Spaces

采样空间（spaces）通常用于指定有效的`action`和`observation`的格式。每个环境都应该有属性 `action_space` 和 `observation_space`，
它们都应该是从 `Space`类继承的类的实例。Gym中有多种可用`space`类型：
- `box`：描述了一个 n 维连续空间。这是一个有界空间，我们可以在其中定义上限和下限，这些上限和下限描述了我们的`observation`可以获得的有效值。
- `Discrete`：描述一个离散空间，其中 {0, 1, …, n-1} 是我们的`observation`或`action`可以采取的可能值。可以使用可选参数将值转换为 {a, a+1, …, a+n-1}。
- `Dict`：表示简单空间的一个字典
- `Tuple`：表示简单空间的一个元组
- `MultiBinary`：创建一个 n 维的二进制空间。参数 n 可以是一个数字或者一个数字列表。
- `MultiDiscrete`：由一系列离散的`action spaces`组成，每个`space`中有不同数量的`action`。


    >>> from gym.spaces import Box, Discrete, Dict, Tuple, MultiBinary, MultiDiscrete
    >>> observation_space = Box(low=-1.0, high=2.0, shape=(3,), dtype=np.float32)
    >>> observation_space.sample()
    [ 1.6952509 -0.4399011 -0.7981693]
    
    >>> observation_space = Discrete(4)
    >>> observation_space.sample()
    1
    
    >>> observation_space = Discrete(5, start=-2)
    >>> observation_space.sample()
    -2
    
    >>> observation_space = Dict({"position": Discrete(2), "velocity": Discrete(3)})
    >>> observation_space.sample()
    OrderedDict([('position', 0), ('velocity', 1)])
    >>>
    >>> observation_space = Tuple((Discrete(2), Discrete(3)))
    >>> observation_space.sample()
    (1, 2)
    >>>
    >>> observation_space = MultiBinary(5)
    >>> observation_space.sample()
    [1 1 1 0 1]
    >>>
    >>> observation_space = MultiDiscrete([ 5, 2, 2 ])
    >>> observation_space.sample()
    [3 0 0]

## Wrappers
`wrappers`是一种无需直接更改底层代码即可修改现有环境的便捷方式。使用`wrappers`可以避免大量样板代码并使您的环境更加模块化。 
`wrappers`也可以链接起来以组合它们的效果。大多数通过 `gym.make` 生成的环境默认情况下已经被`wrap`。

为了`wrap`一个环境，您必须首先初始化一个基环境。
然后你可以将这个环境连同（可能是可选的）参数传递给`wrappers`的构造函数：

    >>> import gym
    >>> from gym.wrappers import RescaleAction
    >>> base_env = gym.make("BipedalWalker-v3")
    >>> base_env.action_space
    Box([-1. -1. -1. -1.], [1. 1. 1. 1.], (4,), float32)
    >>> wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)
    >>> wrapped_env.action_space
    Box([0. 0. 0. 0.], [1. 1. 1. 1.], (4,), float32)
您可能希望`wrappers`执行以下三项非常常见的操作：
- 在将`action`作用于基环境之前进行转换。
- 将基环境返回的`observation`进行转换
- 将基环境返回的`reward`进行转换

通过继承 `ActionWrapper`、`ObservationWrapper`、`RewardWrapper` 并实现相应的转换，可以轻松实现此类`wrapper`。

但是，有时您可能需要实现一个`wrappers`来进行一些更复杂的修改（例如，根据`info`中的数据修改`reward`）。
这样的`wrapper`可以通过继承Wrapper来实现。Gym 已经为你提供了许多常用的`wrapper`。一些例子：
- `TimeLimit`；如果超过最大`timesteps`（或基环境已发出`done`信号），则发出一个`done`信号 
- `ClipAction`：将`action`限幅使其位于`action space`（`Box`型`space`）中
- `RescaleAction`：重新缩放`action`使其位于指定的时间间隔内
- `TimeAwareObservation`：将有关`time-step`的索引的信息添加到`observation`中。在某些情况下有助于确保过渡过程是马尔可夫。

如果你有一个`wrapped`环境，并且你想要在所有`warppers`层下面获得`unwarpped`的环境（以便你可以手动调用一个函数或更改环境的某些底层内容），
你可以使用 `.unwrapped` 属性。 如果环境已经是基环境，`.unwrapped` 属性将只返回自身。

    >>> wrapped_env
    <RescaleAction<TimeLimit<BipedalWalker<BipedalWalker-v3>>>>
    >>> wrapped_env.unwrapped
    <gym.envs.box2d.bipedal_walker.BipedalWalker object at 0x7f87d70712d0>

## Playing within an environment

您还可以使用 `gym.utils.play` 中的`play`功能使用键盘玩游戏。
    
    from gym.utils.play import play
    play(gym.make('Pong-v0'))
这将打开一个环境窗口，并允许您使用键盘控制`agent`。

使用键盘玩游戏需要一个`key-action`映射。 这个映射的类型应该是 `dict[tuple[int], int | None]`，
它将按下的键映射到执行的`action`。 例如，如果同时按下键`w`和空格应该执行`action 2`，那么 `key_to_action` 字典应该如下所示：
    
    {
        ...
        (ord('w'),ord(' ')):2,
        ...
    }
假设我们希望使用左右箭头键来玩 `CartPole-v0`。代码如下：

    import gym
    import pygame
    from gym.utils.play import play

    mapping = {(pygame.K_LEFT,): 0, (pygame.K_RIGHT,): 1}
    play(gym.make("CartPole-v0"), keys_to_action=mapping)

我们从 `pygame` 中获取相应的 `key ID` 常量。 
如果未指定 `key_to_action` 参数，则使用该环境的默认 `key_to_action` 映射（如果提供）。        

此外，如果您希望在玩游戏时绘制实时统计数据，您可以使用 `gym.utils.play.PyPlot`。 
下面是一些示例代码，用于绘制游戏最后 5 秒的奖励：

    def callback(obs_t, obs_tp1, action, rew, done, info):
    return [rew,]
    plotter = PlayPlot(callback, 30 * 5, ["reward"])
    env = gym.make("Pong-v0")
    play(env, callback=plotter.callback)

